{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2.3: \"UNet\"\n",
    "\n",
    "Originally developed by Olaf Ronneberger in MATLAB, the UNet has since been ported to Python and is the industry standard for solving segmentation tasks in the medical machine learning sector. The name is derived from the U-shape of the network. The UNet uses convolutions and pooling to reduce the size of input images and create an information bottleneck. Afterwards, the image is restored to its original size, step by step, utilizing transposed convolutions, which you will have heard about in the presentation of the UNet-Team. Steps on the down- and upwards slope of the U are additionally connected by skip connections, which you have seen before in the ResNet.\n",
    "\n",
    "You can find the original paper here: https://arxiv.org/abs/1505.04597\n",
    "\n",
    "Over the next sessions, we will recreate this milestone architecture in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/datashare/MLCourse/Course_Materials\") # Preferentially import from the datashare.\n",
    "sys.path.append(\"../\") # Otherwise, import from the local folder's parent folder, where your stuff lives.\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch, torch.nn as nn\n",
    "import torchvision, torchvision.transforms as tt\n",
    "import albumentations\n",
    "from torch.multiprocessing import Manager\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "from typing import List\n",
    "\n",
    "from utility import utils as uu\n",
    "from utility.eval import evaluate_segmentation_model\n",
    "from utility.unet import Example_UNet\n",
    "from utility.segloss import ExampleSegmentationLoss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK: Add some data augmentations of your choice (or None, if you want to test something else).\n",
    "\n",
    "Only use the albumentations package for your augmentations. Why? Because albumentations transforms the targets with the same parameters as the original image. This functionality is vital to preserve useful targets for your predictions, and is guaranteed by albumentations without needing to do anything unusual.\n",
    "\n",
    "A typical example looks like this:\n",
    "```\n",
    "augments = albumentations.Compose([\n",
    "    albumentations.RandomCrop(width=256, height=256),\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.RandomBrightnessContrast(p=0.2)\n",
    "])\n",
    "```\n",
    "As you can see, the process is ostensibly the same as with regular torchvision transforms (although some of the names change on occasion). You can find the list of available transforms here: https://albumentations.ai/docs/getting_started/transforms_and_targets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your data augments go here\n",
    "data_augments = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3577ca316a4133b0f9de55dc652860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1bf6b4902d470f9cd1dd356679aef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60981ea7aa114ffdab8ce525074817cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3038 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialization complete.\n"
     ]
    }
   ],
   "source": [
    "# Train, Val, and Test datasets are all contained within this dataset.\n",
    "# They can be selected by setting 'ds.set_mode(selection)'.\n",
    "\n",
    "# We could also cache any data we read from disk to shared memory, or\n",
    "# to regular memory, where each dataloader worker caches the entire\n",
    "# dataset. Option 1 creates more overhead than gain for this problem,\n",
    "# while option 2 requires more memory than we have. Hence, we still\n",
    "# read everything from disk.\n",
    "\n",
    "cache_me = False\n",
    "if cache_me is True:\n",
    "    cache_mgr = Manager()\n",
    "    cache_mgr.data = cache_mgr.dict()\n",
    "    cache_mgr.cached = cache_mgr.dict()\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        cache_mgr.data[k] = cache_mgr.dict()\n",
    "        cache_mgr.cached[k] = False\n",
    "\n",
    "ds = uu.LiTS_Segmentation_Dataset(\n",
    "    #data_dir = \"/home/coder/Course_Materials/data/Clean_LiTS/\",\n",
    "    data_dir = \"../data/Clean_LiTS/\",\n",
    "    transforms = data_augments,\n",
    "    verbose = True,\n",
    "    cache_data = cache_me,\n",
    "    cache_mgr = (cache_mgr if cache_me is True else None),\n",
    "    debug = True,\n",
    ")\n",
    "\n",
    "# This time, our dataset spits out a tensor (our image), and a list of tensors (our targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Play around with the hyperparameters (if you feel like it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default settings\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 5e-6\n",
    "epochs = 10\n",
    "run_name = \"UNet\"\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "time_me = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset = ds, \n",
    "    batch_size = batch_size, \n",
    "    num_workers = 4, \n",
    "    shuffle = True, \n",
    "    drop_last = False, \n",
    "    pin_memory = True,\n",
    "    persistent_workers = (not cache_me),\n",
    "    prefetch_factor = 1\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Construct a UNet.\n",
    "\n",
    "The input dimensions for the network will be the usual B x 1 x 256 x 256. The output dimensions should be B x 3 x 256 x 256. We have three output channels because we will still predict classes 0 (background), 1 (liver) and 2 (liver tumor) - this time, however, we predict the classes on a per-pixel basis.\n",
    "\n",
    "Since our input images have vastly smaller dimensions compared to those used in the original UNet-Paper, we will opt for a different scale of UNet. The general design remains the same as in the paper, except:\n",
    "- We will only downsample 3 times by a factor of 2, using MaxPool (for a minimum resolution 32x32).\n",
    "- Our 3x3 Convolutions will have Padding. Consequently, there will be no cropping during skip connections\n",
    "- We will only have 3 skip connections.\n",
    "- We will go for fewer maximum channels (as we have only 3 downsampling steps, we will have 64, 128, 256, and 512 channels).\n",
    "- Our final output will be 3 channels wide, not 2 (we predict background, liver, and liver tumors).\n",
    "\n",
    "Below, you can find an example UNet. If you want to test your loss module (or anything, really), it should be working with this UNet. Note that the example UNet does **not** follow the exact specifications of this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stand-in example model (if you want to test something else)\n",
    "model = Example_UNet(in_channels = 1, out_classes = 3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation\n",
    "class UNet(torch.nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your model\n",
    "model = UNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Create a DICE/XE loss\n",
    "\n",
    "The loss you create should fulfill the following criteria:\n",
    "- It subclasses torch.nn.module\n",
    "- It is a class that implements an \\_\\_init\\_\\_ function and a forward function.\n",
    "- The forward function takes as argument the predictions from your model, and the target masks from the dataset.\n",
    "- The loss function should compute a Cross-Entropy (XE) loss and a DICE loss, based on predictions and targets and return their sum or weighted sum.\n",
    "\n",
    "Below you will find a regular XE loss, designed for this segmentation task. You can use the example loss module to test your UNet implementation. If your UNet works correctly, this module should work, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an instance of the loss module and we put the loss module onto the GPU aswell.\n",
    "# This is not necessary, but greatly speeds up the computation, if you have the space.\n",
    "# For segmentation tasks, this can be a real time saver.\n",
    "criterion = ExampleSegmentationLoss(classes = 3, weights = torch.Tensor([1, 3, 10])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation\n",
    "class DICE_XE(torch.nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your loss module\n",
    "criterion = DICE_XE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\t Step [1/1108]\t Train Loss: 16.4270\n",
      "Epoch [1/10]\t Step [21/1108]\t Train Loss: 15.5316\n",
      "Epoch [1/10]\t Step [41/1108]\t Train Loss: 15.0102\n",
      "Epoch [1/10]\t Step [61/1108]\t Train Loss: 14.6092\n",
      "Epoch [1/10]\t Step [81/1108]\t Train Loss: 14.0836\n",
      "Epoch [1/10]\t Step [101/1108]\t Train Loss: 13.7912\n",
      "Epoch [1/10]\t Step [121/1108]\t Train Loss: 13.5944\n",
      "Epoch [1/10]\t Step [141/1108]\t Train Loss: 13.3834\n",
      "Epoch [1/10]\t Step [161/1108]\t Train Loss: 13.3071\n",
      "Epoch [1/10]\t Step [181/1108]\t Train Loss: 13.1543\n",
      "Epoch [1/10]\t Step [201/1108]\t Train Loss: 13.0588\n",
      "Epoch [1/10]\t Step [221/1108]\t Train Loss: 13.0050\n",
      "Epoch [1/10]\t Step [241/1108]\t Train Loss: 12.8813\n",
      "Epoch [1/10]\t Step [261/1108]\t Train Loss: 12.7887\n",
      "Epoch [1/10]\t Step [281/1108]\t Train Loss: 12.7793\n",
      "Epoch [1/10]\t Step [301/1108]\t Train Loss: 12.6613\n",
      "Epoch [1/10]\t Step [321/1108]\t Train Loss: 12.5453\n",
      "Epoch [1/10]\t Step [341/1108]\t Train Loss: 12.5138\n",
      "Epoch [1/10]\t Step [361/1108]\t Train Loss: 12.3819\n",
      "Epoch [1/10]\t Step [381/1108]\t Train Loss: 12.3687\n",
      "Epoch [1/10]\t Step [401/1108]\t Train Loss: 12.2585\n",
      "Epoch [1/10]\t Step [421/1108]\t Train Loss: 12.2369\n",
      "Epoch [1/10]\t Step [441/1108]\t Train Loss: 12.1616\n",
      "Epoch [1/10]\t Step [461/1108]\t Train Loss: 12.1116\n",
      "Epoch [1/10]\t Step [481/1108]\t Train Loss: 12.0434\n",
      "Epoch [1/10]\t Step [501/1108]\t Train Loss: 12.0253\n",
      "Epoch [1/10]\t Step [521/1108]\t Train Loss: 11.9292\n",
      "Epoch [1/10]\t Step [541/1108]\t Train Loss: 11.8859\n",
      "Epoch [1/10]\t Step [561/1108]\t Train Loss: 11.8160\n",
      "Epoch [1/10]\t Step [581/1108]\t Train Loss: 11.7769\n",
      "Epoch [1/10]\t Step [601/1108]\t Train Loss: 11.7164\n",
      "Epoch [1/10]\t Step [621/1108]\t Train Loss: 11.6739\n",
      "Epoch [1/10]\t Step [641/1108]\t Train Loss: 11.5765\n",
      "Epoch [1/10]\t Step [661/1108]\t Train Loss: 11.6105\n",
      "Epoch [1/10]\t Step [681/1108]\t Train Loss: 11.5139\n",
      "Epoch [1/10]\t Step [701/1108]\t Train Loss: 11.4594\n",
      "Epoch [1/10]\t Step [721/1108]\t Train Loss: 11.4065\n",
      "Epoch [1/10]\t Step [741/1108]\t Train Loss: 11.3633\n",
      "Epoch [1/10]\t Step [761/1108]\t Train Loss: 11.3173\n",
      "Epoch [1/10]\t Step [781/1108]\t Train Loss: 11.2665\n",
      "Epoch [1/10]\t Step [801/1108]\t Train Loss: 11.2152\n",
      "Epoch [1/10]\t Step [821/1108]\t Train Loss: 11.2207\n",
      "Epoch [1/10]\t Step [841/1108]\t Train Loss: 11.1485\n",
      "Epoch [1/10]\t Step [861/1108]\t Train Loss: 11.1044\n",
      "Epoch [1/10]\t Step [881/1108]\t Train Loss: 11.0570\n",
      "Epoch [1/10]\t Step [901/1108]\t Train Loss: 11.0079\n",
      "Epoch [1/10]\t Step [921/1108]\t Train Loss: 10.9293\n",
      "Epoch [1/10]\t Step [941/1108]\t Train Loss: 10.9302\n",
      "Epoch [1/10]\t Step [961/1108]\t Train Loss: 10.9010\n",
      "Epoch [1/10]\t Step [981/1108]\t Train Loss: 10.8585\n",
      "Epoch [1/10]\t Step [1001/1108]\t Train Loss: 10.8433\n",
      "Epoch [1/10]\t Step [1021/1108]\t Train Loss: 10.7949\n",
      "Epoch [1/10]\t Step [1041/1108]\t Train Loss: 10.7238\n",
      "Epoch [1/10]\t Step [1061/1108]\t Train Loss: 10.6819\n",
      "Epoch [1/10]\t Step [1081/1108]\t Train Loss: 10.6784\n",
      "Epoch [1/10]\t Step [1101/1108]\t Train Loss: 10.6545\n",
      "epoch: 0\n",
      "dice_avg_class_0: 0.0\n",
      "iou_avg_class_0: 0.0\n",
      "precision_avg_class_0: 0.0\n",
      "recall_avg_class_0: 0.0\n",
      "dice_avg_class_1: 0.0\n",
      "iou_avg_class_1: 0.0\n",
      "precision_avg_class_1: 0.0\n",
      "recall_avg_class_1: 0.0\n",
      "dice_avg_class_2: 0.004414725170638849\n",
      "iou_avg_class_2: 0.00221463825018106\n",
      "precision_avg_class_2: 0.00221463825018106\n",
      "recall_avg_class_2: 0.9894702188864207\n",
      "Epoch [2/10]\t Step [1/1108]\t Train Loss: 10.6396\n",
      "Epoch [2/10]\t Step [21/1108]\t Train Loss: 10.6362\n",
      "Epoch [2/10]\t Step [41/1108]\t Train Loss: 10.5897\n",
      "Epoch [2/10]\t Step [61/1108]\t Train Loss: 10.5536\n",
      "Epoch [2/10]\t Step [81/1108]\t Train Loss: 10.5247\n",
      "Epoch [2/10]\t Step [101/1108]\t Train Loss: 10.4722\n",
      "Epoch [2/10]\t Step [121/1108]\t Train Loss: 10.4832\n",
      "Epoch [2/10]\t Step [141/1108]\t Train Loss: 10.4548\n",
      "Epoch [2/10]\t Step [161/1108]\t Train Loss: 10.4299\n",
      "Epoch [2/10]\t Step [181/1108]\t Train Loss: 10.3914\n",
      "Epoch [2/10]\t Step [201/1108]\t Train Loss: 10.3641\n",
      "Epoch [2/10]\t Step [221/1108]\t Train Loss: 10.3581\n",
      "Epoch [2/10]\t Step [241/1108]\t Train Loss: 10.3289\n",
      "Epoch [2/10]\t Step [261/1108]\t Train Loss: 10.3095\n",
      "Epoch [2/10]\t Step [281/1108]\t Train Loss: 10.2619\n",
      "Epoch [2/10]\t Step [301/1108]\t Train Loss: 10.2331\n",
      "Epoch [2/10]\t Step [321/1108]\t Train Loss: 10.2260\n",
      "Epoch [2/10]\t Step [341/1108]\t Train Loss: 10.2072\n",
      "Epoch [2/10]\t Step [361/1108]\t Train Loss: 10.1698\n",
      "Epoch [2/10]\t Step [381/1108]\t Train Loss: 10.1795\n",
      "Epoch [2/10]\t Step [401/1108]\t Train Loss: 10.1709\n",
      "Epoch [2/10]\t Step [421/1108]\t Train Loss: 10.1233\n",
      "Epoch [2/10]\t Step [441/1108]\t Train Loss: 10.1102\n",
      "Epoch [2/10]\t Step [461/1108]\t Train Loss: 10.1231\n",
      "Epoch [2/10]\t Step [481/1108]\t Train Loss: 10.1357\n",
      "Epoch [2/10]\t Step [501/1108]\t Train Loss: 10.0374\n",
      "Epoch [2/10]\t Step [521/1108]\t Train Loss: 10.0650\n",
      "Epoch [2/10]\t Step [541/1108]\t Train Loss: 10.0160\n",
      "Epoch [2/10]\t Step [561/1108]\t Train Loss: 10.0424\n",
      "Epoch [2/10]\t Step [581/1108]\t Train Loss: 10.0191\n",
      "Epoch [2/10]\t Step [601/1108]\t Train Loss: 10.0096\n",
      "Epoch [2/10]\t Step [621/1108]\t Train Loss: 9.9467\n",
      "Epoch [2/10]\t Step [641/1108]\t Train Loss: 9.9469\n",
      "Epoch [2/10]\t Step [661/1108]\t Train Loss: 9.9439\n",
      "Epoch [2/10]\t Step [681/1108]\t Train Loss: 9.8914\n",
      "Epoch [2/10]\t Step [701/1108]\t Train Loss: 9.9342\n",
      "Epoch [2/10]\t Step [721/1108]\t Train Loss: 9.9119\n",
      "Epoch [2/10]\t Step [741/1108]\t Train Loss: 9.9212\n",
      "Epoch [2/10]\t Step [761/1108]\t Train Loss: 9.9135\n",
      "Epoch [2/10]\t Step [781/1108]\t Train Loss: 9.8737\n",
      "Epoch [2/10]\t Step [801/1108]\t Train Loss: 9.8276\n",
      "Epoch [2/10]\t Step [821/1108]\t Train Loss: 9.8514\n",
      "Epoch [2/10]\t Step [841/1108]\t Train Loss: 9.8415\n",
      "Epoch [2/10]\t Step [861/1108]\t Train Loss: 9.8499\n",
      "Epoch [2/10]\t Step [881/1108]\t Train Loss: 9.8296\n",
      "Epoch [2/10]\t Step [901/1108]\t Train Loss: 9.8048\n",
      "Epoch [2/10]\t Step [921/1108]\t Train Loss: 9.8151\n",
      "Epoch [2/10]\t Step [941/1108]\t Train Loss: 9.7978\n",
      "Epoch [2/10]\t Step [961/1108]\t Train Loss: 9.7851\n",
      "Epoch [2/10]\t Step [981/1108]\t Train Loss: 9.7833\n",
      "Epoch [2/10]\t Step [1001/1108]\t Train Loss: 9.7907\n",
      "Epoch [2/10]\t Step [1021/1108]\t Train Loss: 9.7704\n",
      "Epoch [2/10]\t Step [1041/1108]\t Train Loss: 9.7486\n",
      "Epoch [2/10]\t Step [1061/1108]\t Train Loss: 9.7418\n",
      "Epoch [2/10]\t Step [1081/1108]\t Train Loss: 9.7443\n",
      "Epoch [2/10]\t Step [1101/1108]\t Train Loss: 9.7671\n",
      "epoch: 1\n",
      "dice_avg_class_0: 0.0\n",
      "iou_avg_class_0: 0.0\n",
      "precision_avg_class_0: 0.0\n",
      "recall_avg_class_0: 0.0\n",
      "dice_avg_class_1: 0.0\n",
      "iou_avg_class_1: 0.0\n",
      "precision_avg_class_1: 0.0\n",
      "recall_avg_class_1: 0.0\n",
      "dice_avg_class_2: 0.004416587412026886\n",
      "iou_avg_class_2: 0.0022146382503013683\n",
      "precision_avg_class_2: 0.0022146382503013683\n",
      "recall_avg_class_2: 0.9999999996709444\n",
      "Epoch [3/10]\t Step [1/1108]\t Train Loss: 9.7297\n",
      "Epoch [3/10]\t Step [21/1108]\t Train Loss: 9.7138\n",
      "Epoch [3/10]\t Step [41/1108]\t Train Loss: 9.7387\n",
      "Epoch [3/10]\t Step [61/1108]\t Train Loss: 9.6919\n",
      "Epoch [3/10]\t Step [81/1108]\t Train Loss: 9.7123\n",
      "Epoch [3/10]\t Step [101/1108]\t Train Loss: 9.7214\n",
      "Epoch [3/10]\t Step [121/1108]\t Train Loss: 9.7152\n",
      "Epoch [3/10]\t Step [141/1108]\t Train Loss: 9.6648\n",
      "Epoch [3/10]\t Step [161/1108]\t Train Loss: 9.6949\n",
      "Epoch [3/10]\t Step [181/1108]\t Train Loss: 9.6485\n",
      "Epoch [3/10]\t Step [201/1108]\t Train Loss: 9.6757\n",
      "Epoch [3/10]\t Step [221/1108]\t Train Loss: 9.6495\n",
      "Epoch [3/10]\t Step [241/1108]\t Train Loss: 9.6798\n",
      "Epoch [3/10]\t Step [261/1108]\t Train Loss: 9.6505\n",
      "Epoch [3/10]\t Step [281/1108]\t Train Loss: 9.6893\n",
      "Epoch [3/10]\t Step [301/1108]\t Train Loss: 9.6565\n",
      "Epoch [3/10]\t Step [321/1108]\t Train Loss: 9.6588\n",
      "Epoch [3/10]\t Step [341/1108]\t Train Loss: 9.6706\n",
      "Epoch [3/10]\t Step [361/1108]\t Train Loss: 9.6746\n",
      "Epoch [3/10]\t Step [381/1108]\t Train Loss: 9.6548\n",
      "Epoch [3/10]\t Step [401/1108]\t Train Loss: 9.6249\n",
      "Epoch [3/10]\t Step [421/1108]\t Train Loss: 9.6519\n",
      "Epoch [3/10]\t Step [441/1108]\t Train Loss: 9.6380\n",
      "Epoch [3/10]\t Step [461/1108]\t Train Loss: 9.6384\n",
      "Epoch [3/10]\t Step [481/1108]\t Train Loss: 9.6386\n",
      "Epoch [3/10]\t Step [501/1108]\t Train Loss: 9.6281\n",
      "Epoch [3/10]\t Step [521/1108]\t Train Loss: 9.5970\n",
      "Epoch [3/10]\t Step [541/1108]\t Train Loss: 9.6334\n",
      "Epoch [3/10]\t Step [561/1108]\t Train Loss: 9.5751\n",
      "Epoch [3/10]\t Step [581/1108]\t Train Loss: 9.6389\n",
      "Epoch [3/10]\t Step [601/1108]\t Train Loss: 9.6109\n",
      "Epoch [3/10]\t Step [621/1108]\t Train Loss: 9.5707\n",
      "Epoch [3/10]\t Step [641/1108]\t Train Loss: 9.6331\n",
      "Epoch [3/10]\t Step [661/1108]\t Train Loss: 9.5683\n",
      "Epoch [3/10]\t Step [681/1108]\t Train Loss: 9.5964\n",
      "Epoch [3/10]\t Step [701/1108]\t Train Loss: 9.5986\n",
      "Epoch [3/10]\t Step [721/1108]\t Train Loss: 9.6463\n",
      "Epoch [3/10]\t Step [741/1108]\t Train Loss: 9.5781\n",
      "Epoch [3/10]\t Step [761/1108]\t Train Loss: 9.6300\n",
      "Epoch [3/10]\t Step [781/1108]\t Train Loss: 9.6184\n",
      "Epoch [3/10]\t Step [801/1108]\t Train Loss: 9.5777\n",
      "Epoch [3/10]\t Step [821/1108]\t Train Loss: 9.5760\n",
      "Epoch [3/10]\t Step [841/1108]\t Train Loss: 9.5990\n",
      "Epoch [3/10]\t Step [861/1108]\t Train Loss: 9.6229\n",
      "Epoch [3/10]\t Step [881/1108]\t Train Loss: 9.6315\n",
      "Epoch [3/10]\t Step [901/1108]\t Train Loss: 9.6115\n",
      "Epoch [3/10]\t Step [921/1108]\t Train Loss: 9.6014\n",
      "Epoch [3/10]\t Step [941/1108]\t Train Loss: 9.6015\n",
      "Epoch [3/10]\t Step [961/1108]\t Train Loss: 9.6133\n",
      "Epoch [3/10]\t Step [981/1108]\t Train Loss: 9.5948\n",
      "Epoch [3/10]\t Step [1001/1108]\t Train Loss: 9.5782\n",
      "Epoch [3/10]\t Step [1021/1108]\t Train Loss: 9.6161\n",
      "Epoch [3/10]\t Step [1041/1108]\t Train Loss: 9.6114\n",
      "Epoch [3/10]\t Step [1061/1108]\t Train Loss: 9.6042\n",
      "Epoch [3/10]\t Step [1081/1108]\t Train Loss: 9.6084\n",
      "Epoch [3/10]\t Step [1101/1108]\t Train Loss: 9.5820\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Projects/MLCourse_reco/Course_Materials/Chapter_2/UNet.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22444758227d/Projects/MLCourse_reco/Course_Materials/Chapter_2/UNet.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Validation loop\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22444758227d/Projects/MLCourse_reco/Course_Materials/Chapter_2/UNet.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m metrics \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m: epoch}\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22444758227d/Projects/MLCourse_reco/Course_Materials/Chapter_2/UNet.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m metrics\u001b[39m.\u001b[39mupdate(evaluate_segmentation_model(model \u001b[39m=\u001b[39;49m model, dataloader \u001b[39m=\u001b[39;49m dl, device \u001b[39m=\u001b[39;49m device))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22444758227d/Projects/MLCourse_reco/Course_Materials/Chapter_2/UNet.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mm\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mv\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m m, v \u001b[39min\u001b[39;00m metrics\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m m\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m#\u001b[39m\u001b[39m\"\u001b[39m)]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22444758227d/Projects/MLCourse_reco/Course_Materials/Chapter_2/UNet.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m uu\u001b[39m.\u001b[39mcsv_logger(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22444758227d/Projects/MLCourse_reco/Course_Materials/Chapter_2/UNet.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     logfile \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../logs/\u001b[39m\u001b[39m{\u001b[39;00mrun_name\u001b[39m}\u001b[39;00m\u001b[39m_val.csv\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22444758227d/Projects/MLCourse_reco/Course_Materials/Chapter_2/UNet.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     content \u001b[39m=\u001b[39m {m: v \u001b[39mfor\u001b[39;00m m, v \u001b[39min\u001b[39;00m metrics\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m m\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m#\u001b[39m\u001b[39m\"\u001b[39m)},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22444758227d/Projects/MLCourse_reco/Course_Materials/Chapter_2/UNet.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m     first \u001b[39m=\u001b[39m (epoch \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22444758227d/Projects/MLCourse_reco/Course_Materials/Chapter_2/UNet.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m     overwrite \u001b[39m=\u001b[39m (epoch \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22444758227d/Projects/MLCourse_reco/Course_Materials/Chapter_2/UNet.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m         )\n",
      "File \u001b[0;32m/raid/MLCourse_reco/Course_Materials/Chapter_2/../utility/eval.py:189\u001b[0m, in \u001b[0;36mevaluate_segmentation_model\u001b[0;34m(model, dataloader, device, w_l)\u001b[0m\n\u001b[1;32m    186\u001b[0m Seg_Metrics \u001b[39m=\u001b[39m Segmentation_Metrics(classes \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, weights \u001b[39m=\u001b[39m w_l)\n\u001b[1;32m    188\u001b[0m \u001b[39m# Get data and target, throw away any other returns from the dataloader.\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[39mfor\u001b[39;00m data, targets, \u001b[39m*\u001b[39m_ \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/MLCourse/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/MLCourse/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1358\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1359\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1362\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLCourse/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1315\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m   1314\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[0;32m-> 1315\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1316\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1317\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/MLCourse/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1151\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1164\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1165\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLCourse/lib/python3.9/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[1;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/miniconda3/envs/MLCourse/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if time_me is True:\n",
    "    c_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # If we are caching, we now have all data and let the (potentially non-persistent) workers know\n",
    "    if cache_me is True and epoch > 0:\n",
    "        dl.dataset.set_cached(\"train\")\n",
    "        dl.dataset.set_cached(\"val\")\n",
    "    \n",
    "    # Time me\n",
    "    if time_me is True:\n",
    "        e_start = time.time()\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        targets = [target.to(device) for target in targets]\n",
    "        if time_me is True:\n",
    "            c_end = time.time()\n",
    "            if step % 20 == 0:\n",
    "                print(f\"CPU time: {c_end-c_start:.4f}s\")\n",
    "            g_start = time.time()\n",
    "        predictions = model(data)\n",
    "        if time_me is True:\n",
    "            g_end = time.time()\n",
    "            c_start = time.time()\n",
    "        if step % 20 == 0 and time_me is True:\n",
    "            print(f\"GPU time: {g_end-g_start:.4f}s\")\n",
    "        loss = criterion(predictions, targets)\n",
    "        if step % 20 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}]\\t Step [{step+1}/{num_steps}]\\t Train Loss: {loss.item():.4f}\")\n",
    "        uu.csv_logger(\n",
    "            logfile = f\"../logs/{run_name}_train.csv\",\n",
    "            content = {\"epoch\": epoch, \"step\": step, \"loss\": loss.item()},\n",
    "            first = (epoch == 0 and step == 0),\n",
    "            overwrite = (epoch == 0 and step == 0)\n",
    "                )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    metrics = {\"epoch\": epoch}\n",
    "    metrics.update(evaluate_segmentation_model(model = model, dataloader = dl, device = device))\n",
    "    print('\\n'.join([f'{m}: {v}' for m, v in metrics.items() if not m.startswith(\"#\")]))\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {m: v for m, v in metrics.items() if not m.startswith(\"#\")},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )\n",
    "        \n",
    "    if time_me is True:\n",
    "        print(f\"Epoch time: {time.time()-e_start:.4f}s\")\n",
    "\n",
    "# Finally, test time\n",
    "ds.set_mode(\"test\")\n",
    "model.eval()\n",
    "\n",
    "metrics = evaluate_segmentation_model(model = model, dataloader = dl, device = device)\n",
    "print(\"Test-time metrics:\")\n",
    "print('\\n'.join([f'{m}: {v}' for m, v in metrics.items() if not m.startswith(\"#\")]))\n",
    "uu.csv_logger(\n",
    "    logfile = f\"../logs/{run_name}_test.csv\",\n",
    "    content = {m: v for m, v in metrics.items() if not m.startswith(\"#\")},\n",
    "    first = True,\n",
    "    overwrite = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
