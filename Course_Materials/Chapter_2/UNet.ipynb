{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2.3: \"UNet\"\n",
    "\n",
    "Originally developed by Olaf Ronneberger in MATLAB, the UNet has since been ported to Python and is the industry standard for solving segmentation tasks in the medical machine learning sector. The name is derived from the U-shape of the network. The UNet uses convolutions and pooling to reduce the size of input images and create an information bottleneck. Afterwards, the image is restored to its original size, step by step, utilizing transposed convolutions, which you will have heard about in the presentation of the UNet-Team. Steps on the down- and upwards slope of the U are additionally connected by skip connections, which you have seen before in the ResNet.\n",
    "\n",
    "You can find the original paper here: https://arxiv.org/abs/1505.04597\n",
    "\n",
    "Over the next sessions, we will recreate this milestone architecture in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/datashare/MLCourse/Course_Materials\") # Preferentially import from the datashare.\n",
    "sys.path.append(\"../\") # Otherwise, import from the local folder's parent folder, where your stuff lives.\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch, torch.nn as nn\n",
    "import torchvision, torchvision.transforms as tt\n",
    "import albumentations\n",
    "from torch.multiprocessing import Manager\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "from typing import List\n",
    "\n",
    "from utility import utils as uu\n",
    "from utility.eval import evaluate_segmentation_model\n",
    "from utility.unet import Example_UNet\n",
    "from utility.segloss import ExampleSegmentationLoss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK: Add some data augmentations of your choice (or None, if you want to test something else).\n",
    "\n",
    "Only use the albumentations package for your augmentations. Why? Because albumentations transforms the targets with the same parameters as the original image. This functionality is vital to preserve useful targets for your predictions, and is guaranteed by albumentations without needing to do anything unusual.\n",
    "\n",
    "A typical example looks like this:\n",
    "```\n",
    "augments = albumentations.Compose([\n",
    "    albumentations.RandomCrop(width=256, height=256),\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.RandomBrightnessContrast(p=0.2)\n",
    "])\n",
    "```\n",
    "As you can see, the process is ostensibly the same as with regular torchvision transforms (although some of the names change on occasion). You can find the list of available transforms here: https://albumentations.ai/docs/getting_started/transforms_and_targets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your data augments go here\n",
    "data_augments = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Val, and Test datasets are all contained within this dataset.\n",
    "# They can be selected by setting 'ds.set_mode(selection)'.\n",
    "\n",
    "# We could also cache any data we read from disk to shared memory, or\n",
    "# to regular memory, where each dataloader worker caches the entire\n",
    "# dataset. Option 1 creates more overhead than gain for this problem,\n",
    "# while option 2 requires more memory than we have. Hence, we still\n",
    "# read everything from disk.\n",
    "\n",
    "cache_me = False\n",
    "if cache_me is True:\n",
    "    cache_mgr = Manager()\n",
    "    cache_mgr.data = cache_mgr.dict()\n",
    "    cache_mgr.cached = cache_mgr.dict()\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        cache_mgr.data[k] = cache_mgr.dict()\n",
    "        cache_mgr.cached[k] = False\n",
    "\n",
    "ds = uu.LiTS_Segmentation_Dataset(\n",
    "    #data_dir = \"/home/coder/Course_Materials/data/Clean_LiTS/\",\n",
    "    data_dir = \"../data/Clean_LiTS/\",\n",
    "    transforms = data_augments,\n",
    "    verbose = True,\n",
    "    cache_data = cache_me,\n",
    "    cache_mgr = (cache_mgr if cache_me is True else None),\n",
    "    debug = True,\n",
    ")\n",
    "\n",
    "# This time, our dataset spits out a tensor (our image), and a list of tensors (our targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Play around with the hyperparameters (if you feel like it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default settings\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 5e-6\n",
    "epochs = 10\n",
    "run_name = \"UNet\"\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "time_me = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset = ds, \n",
    "    batch_size = batch_size, \n",
    "    num_workers = 4, \n",
    "    shuffle = True, \n",
    "    drop_last = False, \n",
    "    pin_memory = True,\n",
    "    persistent_workers = (not cache_me),\n",
    "    prefetch_factor = 1\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Construct a UNet.\n",
    "\n",
    "The input dimensions for the network will be the usual B x 1 x 256 x 256. The output dimensions should be B x 3 x 256 x 256. We have three output channels because we will still predict classes 0 (background), 1 (liver) and 2 (liver tumor) - this time, however, we predict the classes on a per-pixel basis.\n",
    "\n",
    "Since our input images have vastly smaller dimensions compared to those used in the original UNet-Paper, we will opt for a different scale of UNet. The general design remains the same as in the paper, except:\n",
    "- We will only downsample 3 times by a factor of 2, using MaxPool (for a minimum resolution 32x32).\n",
    "- Our 3x3 Convolutions will have Padding. Consequently, there will be no cropping during skip connections\n",
    "- We will only have 3 skip connections.\n",
    "- We will go for fewer maximum channels (as we have only 3 downsampling steps, we will have 64, 128, 256, and 512 channels).\n",
    "- Our final output will be 3 channels wide, not 2 (we predict background, liver, and liver tumors).\n",
    "\n",
    "Below, you can find an example UNet. If you want to test your loss module (or anything, really), it should be working with this UNet. Note that the example UNet does **not** follow the exact specifications of this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stand-in example model (if you want to test something else)\n",
    "model = Example_UNet(in_channels = 1, out_classes = 3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation\n",
    "class UNet(torch.nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your model\n",
    "model = UNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Create a DICE/XE loss\n",
    "\n",
    "The loss you create should fulfill the following criteria:\n",
    "- It subclasses torch.nn.module\n",
    "- It is a class that implements an \\_\\_init\\_\\_ function and a forward function.\n",
    "- The forward function takes as argument the predictions from your model, and the target masks from the dataset.\n",
    "- The loss function should compute a Cross-Entropy (XE) loss and a DICE loss, based on predictions and targets and return their sum or weighted sum.\n",
    "- The loss should accept a tensor of shape $B*3*H*W$ as predictions, and a tensor of shape $B*2*H*W$ as targets, where B is the batch size, and H and W are height and width of the images. The predictions have a channel dimension of length 3, because we predict background, liver, and tumor regions. Targets only have a channel dimension of length 2 because we only get segmentations for liver and tumor regions - we will derive our background from the other two segmentations.\n",
    "- Note that the class you write should not use torch.nn.CrossEntropyLoss under the hood - we want to write the calculation of the loss ourselves.\n",
    "\n",
    "Below you will find a regular XE loss, designed for this segmentation task. You can use the example loss module to test your UNet implementation. If your UNet works correctly, this module should work, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an instance of the loss module and we put the loss module onto the GPU aswell.\n",
    "# This is not necessary, but greatly speeds up the computation, if you have the space.\n",
    "# For segmentation tasks, this can be a real time saver.\n",
    "criterion = ExampleSegmentationLoss(\n",
    "    classes = 3,\n",
    "    weights = torch.Tensor([1, 3, 10]),\n",
    "    on_the_fly_background = True,\n",
    "    allow_multiclass = False\n",
    "    )\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation\n",
    "class DICE_XE_Loss:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your loss module\n",
    "criterion = DICE_XE_Loss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_me is True:\n",
    "    c_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # If we are caching, we now have all data and let the (potentially non-persistent) workers know\n",
    "    if cache_me is True and epoch > 0:\n",
    "        dl.dataset.set_cached(\"train\")\n",
    "        dl.dataset.set_cached(\"val\")\n",
    "    \n",
    "    # Time me\n",
    "    if time_me is True:\n",
    "        e_start = time.time()\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        targets = [target.to(device) for target in targets]\n",
    "        if time_me is True:\n",
    "            c_end = time.time()\n",
    "            if step % 20 == 0:\n",
    "                print(f\"CPU time: {c_end-c_start:.4f}s\")\n",
    "            g_start = time.time()\n",
    "        predictions = model(data)\n",
    "        if time_me is True:\n",
    "            g_end = time.time()\n",
    "            c_start = time.time()\n",
    "        if step % 20 == 0 and time_me is True:\n",
    "            print(f\"GPU time: {g_end-g_start:.4f}s\")\n",
    "        loss = criterion(predictions, targets)\n",
    "        if step % 20 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}]\\t Step [{step+1}/{num_steps}]\\t Train Loss: {loss.item():.4f}\")\n",
    "        uu.csv_logger(\n",
    "            logfile = f\"../logs/{run_name}_train.csv\",\n",
    "            content = {\"epoch\": epoch, \"step\": step, \"loss\": loss.item()},\n",
    "            first = (epoch == 0 and step == 0),\n",
    "            overwrite = (epoch == 0 and step == 0)\n",
    "                )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    metrics = {\"epoch\": epoch}\n",
    "    metrics.update(evaluate_segmentation_model(model = model, dataloader = dl, device = device))\n",
    "    print('\\n'.join([f'{m}: {v}' for m, v in metrics.items() if not m.startswith(\"#\")]))\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {m: v for m, v in metrics.items() if not m.startswith(\"#\")},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )\n",
    "        \n",
    "    if time_me is True:\n",
    "        print(f\"Epoch time: {time.time()-e_start:.4f}s\")\n",
    "\n",
    "# Finally, test time\n",
    "ds.set_mode(\"test\")\n",
    "model.eval()\n",
    "\n",
    "metrics = evaluate_segmentation_model(model = model, dataloader = dl, device = device)\n",
    "print(\"Test-time metrics:\")\n",
    "print('\\n'.join([f'{m}: {v}' for m, v in metrics.items() if not m.startswith(\"#\")]))\n",
    "uu.csv_logger(\n",
    "    logfile = f\"../logs/{run_name}_test.csv\",\n",
    "    content = {m: v for m, v in metrics.items() if not m.startswith(\"#\")},\n",
    "    first = True,\n",
    "    overwrite = True\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "If you have some time left and your model has successfully trained, try outputting a few images and the segmentations your model predicted, as well as the ground truth. Does your model do a good job? Is there something your model is particularly good or bad at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
