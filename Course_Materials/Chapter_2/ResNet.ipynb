{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2.2: \"ResNet\"\n",
    "\n",
    "ResNet, developed by He et al. in 2015, is one of the most well-known and -cited architectures out there. It is quite similar to previous convolutional neural networks, but introduces several new features which have since been used in almost every newly emerging architectures:\n",
    "- Residual connections. In a particularly deep neural network, information can get lost, if the gradients are too small or too large, and potentially useful information gathered in the first few convolution layers cannot be recalled efficiently in later layers. Residual connections (a.k.a. skip connections) offer a solution. For every \"block\" of convolutions and other operations, the final output is the convolved image plus the original input. This way, if the neural network does not \"need\" a later layer, or decides that the best result will be achieved, by only propagating the information it already had into the next block, it can now do so very easily.\n",
    "- Batch Normalization. Normalization is a common feature even in older neural network architectures. In BatchNorm, the normalization layer normalizes all inputs so that the following layer receives an input with a fixed mean and variance. The idea behind this process is as follows. Let's say layer 1 produces the output 0-0-0-1 if the input image contains a dog, and 0-0-1-1 if it contains a cat. Layer 2, containing only 2 neurons (one for dog, one for cat) should therefore assign a large weight to signals from the third neuron of layer 1 to its cat neuron. Now both layers are optimized during a backward pass. Suddenly, layer 1 produces outputs like 0-1-0-1 and 0-1-0-0. Even if layer 2 made a good correction, its optimization goal is a kind of moving target, which it will occasionally miss (because the output distribution of the previous layer might have changed). This problem is known as the *internal covariate shift*. By fixing the mean and variance of the distribution, the layer behind the BatchNorm should see a much weaker or even negligible covariate shift. (Recently, however, it has been suggested that BatchNorm works for different reasons, and it is still not fully clear why it works so well.)\n",
    "\n",
    "You can find the original paper here: https://arxiv.org/abs/1512.03385\n",
    "\n",
    "Over the next sessions, we will recreate this milestone architecture in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/datashare/MLCourse/Course_Materials\") # Preferentially import from the datashare.\n",
    "sys.path.append(\"../\") # Otherwise, import from the local folder's parent folder, where your stuff lives.\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch, torch.nn as nn\n",
    "import torchvision, torchvision.transforms as tt\n",
    "from torch.multiprocessing import Manager\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "\n",
    "from utility import utils as uu\n",
    "from utility.eval import evaluate_classifier_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Add some data augmentations of your choice (or None, if you want to test something else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your data augments go here\n",
    "data_augments = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Val, and Test datasets are all contained within this dataset.\n",
    "# They can be selected by setting 'ds.set_mode(selection)'.\n",
    "\n",
    "# We could also cache any data we read from disk to shared memory, or\n",
    "# to regular memory, where each dataloader worker caches the entire\n",
    "# dataset. Option 1 creates more overhead than gain for this problem,\n",
    "# while option 2 requires more memory than we have. Hence, we still\n",
    "# read everything from disk.\n",
    "\n",
    "cache_me = False\n",
    "if cache_me is True:\n",
    "    cache_mgr = Manager()\n",
    "    cache_mgr.data = cache_mgr.dict()\n",
    "    cache_mgr.cached = cache_mgr.dict()\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        cache_mgr.data[k] = cache_mgr.dict()\n",
    "        cache_mgr.cached[k] = False\n",
    "\n",
    "ds = uu.LiTS_Classification_Dataset(\n",
    "    data_dir = \"/home/coder/Course_Materials/data/Clean_LiTS/\",\n",
    "    transforms = data_augments,\n",
    "    verbose = True,\n",
    "    cache_data = cache_me,\n",
    "    cache_mgr = (cache_mgr if cache_me is True else None),\n",
    "    debug = True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Play around with the hyperparameters (if you feel like it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default settings\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 5e-6\n",
    "epochs = 10\n",
    "run_name = \"ResNet\"\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "time_me = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset = ds, \n",
    "    batch_size = batch_size, \n",
    "    num_workers = 4, \n",
    "    shuffle = True, \n",
    "    drop_last = False, \n",
    "    pin_memory = True,\n",
    "    persistent_workers = (not cache_me),\n",
    "    prefetch_factor = 1\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Construct a ResNet-50 (this one you have to do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stand-in example model (if you want to test something else)\n",
    "model = torchvision.models.resnet18()\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation\n",
    "class ResNet50(torch.nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your model\n",
    "model = ResNet50()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_me is True:\n",
    "    c_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # If we are caching, we now have all data and let the (potentially non-persistent) workers know\n",
    "    if cache_me is True and epoch > 0:\n",
    "        dl.dataset.set_cached(\"train\")\n",
    "        dl.dataset.set_cached(\"val\")\n",
    "    \n",
    "    # Time me\n",
    "    if time_me is True:\n",
    "        e_start = time.time()\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        if time_me is True:\n",
    "            c_end = time.time()\n",
    "            if step % 100 == 0:\n",
    "                print(f\"CPU time: {c_end-c_start:.4f}s\")\n",
    "            g_start = time.time()\n",
    "        predictions = model(data)\n",
    "        if time_me is True:\n",
    "            g_end = time.time()\n",
    "            c_start = time.time()\n",
    "        if step % 100 == 0 and time_me is True:\n",
    "            print(f\"GPU time: {g_end-g_start:.4f}s\")\n",
    "        loss = criterion(predictions, targets)\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}]\\t Step [{step+1}/{num_steps}]\\t Train Loss: {loss.item():.4f}\")\n",
    "        uu.csv_logger(\n",
    "            logfile = f\"../logs/{run_name}_train.csv\",\n",
    "            content = {\"epoch\": epoch, \"step\": step, \"loss\": loss.item()},\n",
    "            first = (epoch == 0 and step == 0),\n",
    "            overwrite = (epoch == 0 and step == 0)\n",
    "                )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    val_accuracy, avg_val_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\\t Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.4f}\")\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {\"epoch\": epoch, \"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )\n",
    "        \n",
    "    if time_me is True:\n",
    "        print(f\"Epoch time: {time.time()-e_start:.4f}s\")\n",
    "\n",
    "# Finally, test time\n",
    "ds.set_mode(\"test\")\n",
    "model.eval()\n",
    "\n",
    "test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "print(f\"Epoch [{epoch+1}/{epochs}]\\t Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}\")\n",
    "uu.csv_logger(\n",
    "    logfile = f\"../logs/{run_name}_test.csv\",\n",
    "    content = {\"epoch\": epoch, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy},\n",
    "    first = True,\n",
    "    overwrite = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "737ba72d4981e532feb2a3ef7367f32bbcc5cce95e990d6dfd214e053b52017d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
