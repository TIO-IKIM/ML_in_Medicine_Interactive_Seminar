{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2.4: \"Vision Transformer\"\n",
    "\n",
    "The Vision Transformer (ViT) was developed by Dosovitskiy et al in 2020. It was adapted to vision tasks after the original Transformer, in yet another milestone paper by Vaswani et al in 2017, rapidly rose to become the state of the art in Natural Language Processing (NLP).\n",
    "\n",
    "The original core idea of the Transformer was to split sentences apart into tokens, and then perform some \"embedding\", to translate it into a more efficient machine language, if you will. Afterwards, the novel \"(Self-)Attention\" mechanism is applied. It consists of so-called **Q**ueries, **K**eys, and **V**alues. We do the following:\n",
    "\n",
    "0) First, we generate ourselves Queries (Q), Keys (K) and Values (V). These are the output of a (learnable) linear projection from our embedded image patches. \n",
    "In a sense, you can understand Queries as questions or searches that the neural network wants to ask/perform on the image patches. As a simple analogy, consider YouTube. Your query is the thing you type into the search bar. In this analogy, the keys are the categories of content that YouTube's database internally uses, and the values to the keys are the corresponding videos of each category.\n",
    "\n",
    "1) The neural network compares the Queries against the Keys and computes which Keys the Query aligns with the most. Imagine typing a search into YouTube and getting back the closest thing that matches your search.  \n",
    "Mathematically, this is just a matrix multiplication:  \n",
    "$e_{q,k_{i}} = q * k_{i}$.\n",
    "\n",
    "2) The neural network \"decides\" which of the keys corresponding to the query or queries were most relevant. In this analogy, this would be YouTube deciding what request that it can perform most closely resembles what your query was, and discarding or deprioritizing the worse ones.  \n",
    "This is done with a Softmax operation:  \n",
    "$a_{q,k_{i}} = softmax(e_{q,k_{i}})$.\n",
    "This also introduces a non-linearity between step 1 and 3, which we already know is necessary.\n",
    "\n",
    "3) Finally, it \"retrieves\" the values corresponding to the keys. In our analogy, this would be YouTube collecting all videos corresponding to the keys which best aligned our query.  \n",
    "Once again, this sounds mysterious, but is only a matrix multiplication:  \n",
    "$attention(q, k, v) = \\sum_{i}{a_{q,k_{i}} * v_{k_{i}}}$.\n",
    "\n",
    "ViT, in order to apply the same techniques on images, splits the input image into patches of size 16x16. Just like the tokens created from words for the original Transformer model, these patches are embedded, too (in the case of images such an embedding takes the same form as other operations we have seen in the past - convolutions, linear layers, etc.). Once again, the Queries-Keys-Values approach can be applied.\n",
    "\n",
    "You can find the original Transformer paper here: https://arxiv.org/pdf/1706.03762.pdf\n",
    "and the Vision Transformer here: https://arxiv.org/pdf/2010.11929.pdf\n",
    "\n",
    "Over the next sessions, we will recreate this milestone architecture in PyTorch. Note that for this task, we will provide some additional guide rails and lift some restrictions: \n",
    "- You do not have to implement the vision transformer like it is implemented in the original paper or in PyTorch. All we want is for you to create a functional vision transformer that has a) Patching, b) some sort of Embedding, c) some sort of Multi-Head Self-Attention, d) some sort of structure which applies the Self-Attention block multiple times. \n",
    "- You do not have to produce great results - Vision Transformers are notorious for the amount of data they normally need to train successfully, and they are typically larger than what you can reasonably train on the GPUs available to you. If your code runs without crashing and we can see some form of improvement during training, we consider the task solved successfully.\n",
    "- We will return to the task of **classification**. Semantic segmentation with a Vision Transformer is definitely possible, and has been shown to be competitive or even SOTA on segmentation problems, but such models are quite hard to implement and debug, generally require pretraining which we do not have for our custom models, are not particularly fast, and, most importantly, have many parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/datashare/MLCourse/Course_Materials\") # Preferentially import from the datashare.\n",
    "sys.path.append(\"../\") # Otherwise, import from the local folder's parent folder, where your stuff lives.\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch, torch.nn as nn\n",
    "import torchvision, torchvision.transforms as tt\n",
    "from torch.multiprocessing import Manager\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "\n",
    "from utility import utils as uu\n",
    "from utility.eval import evaluate_classifier_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Add some data augmentations of your choice (or None, if you want to test something else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your data augments go here\n",
    "data_augments = tt.Compose(\n",
    "    [\n",
    "        tt.RandomHorizontalFlip(p = 0.5),\n",
    "        #tt.Resize((224, 224))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Val, and Test datasets are all contained within this dataset.\n",
    "# They can be selected by setting 'ds.set_mode(selection)'.\n",
    "\n",
    "# We could also cache any data we read from disk to shared memory, or\n",
    "# to regular memory, where each dataloader worker caches the entire\n",
    "# dataset. Option 1 creates more overhead than gain for this problem,\n",
    "# while option 2 requires more memory than we have. Hence, we still\n",
    "# read everything from disk.\n",
    "\n",
    "cache_me = False\n",
    "if cache_me is True:\n",
    "    cache_mgr = Manager()\n",
    "    cache_mgr.data = cache_mgr.dict()\n",
    "    cache_mgr.cached = cache_mgr.dict()\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        cache_mgr.data[k] = cache_mgr.dict()\n",
    "        cache_mgr.cached[k] = False\n",
    "\n",
    "ds = uu.LiTS_Classification_Dataset(\n",
    "    #data_dir = \"/home/coder/Course_Materials/data/Clean_LiTS/\",\n",
    "    data_dir = \"../data/Clean_LiTS/\",\n",
    "    transforms = data_augments,\n",
    "    verbose = True,\n",
    "    cache_data = cache_me,\n",
    "    cache_mgr = (cache_mgr if cache_me is True else None),\n",
    "    debug = True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Play around with the hyperparameters (if you feel like it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default settings\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-6\n",
    "epochs = 10\n",
    "run_name = \"ViT\"\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "time_me = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset = ds, \n",
    "    batch_size = batch_size, \n",
    "    num_workers = 4, \n",
    "    shuffle = True, \n",
    "    drop_last = False, \n",
    "    pin_memory = True,\n",
    "    persistent_workers = (not cache_me),\n",
    "    prefetch_factor = 1\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Construct a Vision Transformer (this one you have to do).\n",
    "\n",
    "#### The model class\n",
    "As this one is a little more difficult, we will guide you through the construction of the Vision Transformer (ViT).\n",
    "We will construct the ViT using two classes, one for the model itself, and one for its principal component, the\n",
    "Transformer Encoder Block.\n",
    "\n",
    "Let's first look at what the model does in general. As per usual, we will need an \\_\\_init\\_\\_ method, and a forward method\n",
    "which takes a tensor and returns a tensor. Our input tensor has the size $[B * 1 * 256 * 256]$, with $B$ being the batch size.\n",
    "Our output size should be $[B * 3]$, for the three classes. We will guide you through the steps of the forward pass, and anything you need in the \\_\\_init\\_\\_ function is up to you to add to it as you see fit.  \n",
    "1) First, we need to \"embed\" our image(s). To do this, we want to cut up our image into patches. When you read the ViT paper,\n",
    "you can see the patch size that is commonly used. Each of these image patches must also be one-dimensional. This means that\n",
    "whatever you use for embedding must perform a tensor operation that changes its size like this:  \n",
    "$[B * 1 * 256 * 256] \\rightarrow^a [B * EL * SX * SY] \\rightarrow^b [B * EL * SL] \\rightarrow^c [B * SL * EL]$ where $EL$ means embedding length (a value that you can choose), and  \n",
    "$SL = SX * SY = H_{image}/L_{patch} * W_{image}/L_{patch}$ is the length of the sequence of patches you got, given your patch size.  \n",
    "Without going into too much detail, the reason that we do steps b and c relates to computational performance.  \n",
    "  \n",
    "2) Next, we attach something called a class token, or *cls token* to our tensor. What is it, and why do we add it? In essence, our tensor is a somewhat compressed and warped representation of our original image. Later, we want each embedded image patch to be able to \"see\" every other patch. As an analogy, imagine you have a patch that is largely blue. It might want to check for each patch if that patch contains something vaguely fish-shaped - if yes, the patch could get the info that its likely underwater, if not, its likely in the sky. This is something that the Self Attention mechanism will let us do later, and we will discuss it in more detail there. However, we note one important thing - as we drag the image patches through the various layers of our network, we do not want them to lose their original information. So where do we put information that a patch has \"learned\" from other patches?  \n",
    "The solution here is to add one extra patch to the front of our sequence, the aforementioned *cls token* (or maybe cls patch is a better term for images). This patch is not an extra part of our image, but we will treat it as one. It starts off empty, but it also gets to learn info from other patches. The idea is that whatever conclusions the network derives are aggregated in this token. We are not forced to \"lose\" information, because there was never anything in this patch.  \n",
    "So, what is your task? Create a learnable parameter called 'cls_token' and tape it to the front end of your sequence. This should change the size of your tensor like this:  \n",
    "$[B * SL * EL] \\rightarrow [B * SL+1 * EL]$.  \n",
    "  \n",
    "3) Now we would apply the TransformerEncoderBlocks in the forward pass. As it is by far the most difficult feature, let's ignore it for now. For every TransformerEncoderBlock we would want (see the paper), just put down a nn.Identity() layer - just like the Identity block, the TransformerEncoderBlock's input and output shapes are going to be the same.  \n",
    "  \n",
    "4) Next, we toss out *every* part of our sequence except the *cls token*. Wait, WHAT?! Yeah. As it turns out, that empty \"fake image patch\" we added to our sequence is capable of aggregating enough info that using only it as input of our final layer is basically just as good as using the entire sequence. However, using only the cls token saves us a lot of parameters and therefore a lot of time, several orders of magnitude in fact. The tensor shape should change like this during this step:  \n",
    "$[B * SL * EL] \\rightarrow [B * 1 * EL]$.  \n",
    "Alternatively, you could also aggregate the information from all patches using a pooling function. In the end, the important thing is that we bring our tensor into a shape usable by our final linear layer.  \n",
    "  \n",
    "5) Finally, we throw in our MLP (MultiLayerPerceptron, basically one or several linear layers) head. The paper uses a LayerNorm, flattens the tensor in every dimension except the batch dimension, and finally applies a Linear layer, at the end of which we get our classification predictions. The tensor shape should change as follows during these steps:  \n",
    "$[B * 1 * EL] or [B * SL+1 * EL]\\rightarrow^a [B * EL] \\rightarrow^b [B * 3]$.  \n",
    "  \n",
    "Try checking the size parameter of your tensor in the forward pass along the way. If it did what the equations wanted, then you've completed the first half of the task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ):\n",
    "\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The TransformerEncoder block\n",
    "Now we need to build a class that performs Self-Attention. As with any other module, we inherit from torch.nn.Module. Again, we will construct the forward pass step by step, and you can fill in anything you need in the \\_\\_init\\_\\_ method.  \n",
    "  \n",
    "0) We start off with a LayerNorm layer, as the paper suggests and keep a copy of x for our first skip connection.\n",
    "\n",
    "1) The first actual step of a TransformerEncoderBlock is generating the $Q$, $K$, and $V$ tensors. They are created from x using a linear projection and should have the shape $[B * SL + 1 * NH * EL/NH]$, where $B$ is the batch size, $NH$ is the number of heads of our multi-head attention and $EL$ is our embedding length from before. We also keep a copy of the original tensor around for the purpose of a residual connection. Finally, we need to permute the tensor dimensions, so that our $Q$, $K$ and $V$ have a shape of  \n",
    "$[B * NH * SL + 1 * EL/NH]$.  \n",
    "  \n",
    "2) Now we calculate our Self-Attention according to the formula from Vaswani et al. 2017:  \n",
    "$out = \\frac{1}{\\sqrt{d_k}} * softmax(Q * K^{T}) * V$, where $d_k = EL/NH$.  \n",
    "Your output should be of shape $[B * NH * SL + 1 * EL/NH]$.  \n",
    "We permute the tensor back to the shape $[B * SL + 1 * NH * EL/NH]$  \n",
    "and reshape it to combine the last two dimensions back into one dimension of size $EL$.\n",
    "  \n",
    "3) Finally, we add on top of that a LayerNorm, a linear layer, a nonlinear activation function, and add the skip connection from before the attention block, in that order.  \n",
    "  \n",
    "4) We repeat the structure from above once more, only that this time the residual connection skips from after the first to after the second linear layer.\n",
    "\n",
    "5) There is different implementations out there where the individual skip connection i is added with a corresponding scaling factor $\\gamma_i$, which is also a learnable parameter. Try for yourself whether this is helpful!\n",
    "  \n",
    "And that's it already! :)\n",
    "Try testing whether your TransformerEncoder block can propagate a test tensor with its forward function. If yes, and the tensor shape remains the same as before, that's a good sign. Hint: It is a good idea to use a tensor that has unusual numbers for its shape so that you always know which part of the shape comes from where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ):\n",
    "        \n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your model\n",
    "model = VisionTransformer( ... )\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: The original paper uses SGD with momentum 0.9 and varying learning rates. Try it out and see if it can beat Adam(W)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, weight_decay = weight_decay, momentum = 0.9)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, when the ViT trains, and you want to check for potentially improved results, let it train some time. If the results are underwhelming, it may very well be due to ViTs being both overkill and under-equipped (in terms of data) for the task; ViTs are notoriously data-hungry, and compared to the industry standard, medical datasets are typically quite small.\n",
    "\n",
    "**tl;dr** - If your ResNet had 97% accuracy, and your ViT has 92%, this is fine. Your ViT works. If your ViT cannot get past 80% (or, suspiciously, always achieves 68.71%, you probably made a mistake with your implementation somewhere)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_me is True:\n",
    "    c_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # If we are caching, we now have all data and let the (potentially non-persistent) workers know\n",
    "    if cache_me is True and epoch > 0:\n",
    "        dl.dataset.set_cached(\"train\")\n",
    "        dl.dataset.set_cached(\"val\")\n",
    "    \n",
    "    # Time me\n",
    "    if time_me is True:\n",
    "        e_start = time.time()\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        if time_me is True:\n",
    "            c_end = time.time()\n",
    "            if step % 100 == 0:\n",
    "                print(f\"CPU time: {c_end-c_start:.4f}s\")\n",
    "            g_start = time.time()\n",
    "        predictions = model(data)\n",
    "        if time_me is True:\n",
    "            g_end = time.time()\n",
    "            c_start = time.time()\n",
    "        if step % 100 == 0 and time_me is True:\n",
    "            print(f\"GPU time: {g_end-g_start:.4f}s\")\n",
    "        loss = criterion(predictions, targets)\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}]\\t Step [{step+1}/{num_steps}]\\t Train Loss: {loss.item():.4f}\")\n",
    "        uu.csv_logger(\n",
    "            logfile = f\"../logs/{run_name}_train.csv\",\n",
    "            content = {\"epoch\": epoch, \"step\": step, \"loss\": loss.item()},\n",
    "            first = (epoch == 0 and step == 0),\n",
    "            overwrite = (epoch == 0 and step == 0)\n",
    "                )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    val_accuracy, avg_val_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\\t Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.4f}\")\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {\"epoch\": epoch, \"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )\n",
    "        \n",
    "    if time_me is True:\n",
    "        print(f\"Epoch time: {time.time()-e_start:.4f}s\")\n",
    "\n",
    "# Finally, test time\n",
    "ds.set_mode(\"test\")\n",
    "model.eval()\n",
    "\n",
    "test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "print(f\"Epoch [{epoch+1}/{epochs}]\\t Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}\")\n",
    "uu.csv_logger(\n",
    "    logfile = f\"../logs/{run_name}_test.csv\",\n",
    "    content = {\"epoch\": epoch, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy},\n",
    "    first = True,\n",
    "    overwrite = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "737ba72d4981e532feb2a3ef7367f32bbcc5cce95e990d6dfd214e053b52017d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
